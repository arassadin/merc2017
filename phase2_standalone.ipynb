{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=-1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(sequence, every=None, size=None, name='Items'):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(arr, mins, maxs):\n",
    "    arr = arr.astype(float)\n",
    "    for i in range(arr.shape[-1]):\n",
    "        arr[:, :, i] -= mins[i]\n",
    "        arr[:, :, i] /= maxs[i]\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mins_audio, maxs_audio = np.load('data/audio_mins.npy'), np.load('data/audio_maxs.npy')\n",
    "mins_kinect, maxs_kinect = np.load('data/kinect_mins.npy'), np.load('data/kinect_maxs.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tmp = keras.models.load_model('model_audio.h5')\n",
    "model_audio = keras.models.Model(model_tmp.input, model_tmp.get_layer('dropout_2').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tmp = keras.models.load_model('model_eyes.h5')\n",
    "model_eyes = keras.models.Model(model_tmp.input, model_tmp.get_layer('dropout_2').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tmp = keras.models.load_model('model_face.h5')\n",
    "model_face = keras.models.Model(model_tmp.input, model_tmp.get_layer('dropout_2').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tmp = keras.models.load_model('model_kinect.h5')\n",
    "model_kinect = keras.models.Model(model_tmp.input, model_tmp.get_layer('dropout_2').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2114, 400, 36) (2114, 200, 6) (2114, 200, 100) (2114, 60, 27)\n"
     ]
    }
   ],
   "source": [
    "X_val_audio = norm(np.load('X_val_audio.npy'), mins_audio, maxs_audio)\n",
    "X_val_eyes = np.load('X_val_eyes.npy')\n",
    "X_val_face = np.load('X_val_face.npy')\n",
    "X_val_kinect = norm(np.load('X_val_kinect.npy'), mins_kinect, maxs_kinect)\n",
    "\n",
    "print X_val_audio.shape, X_val_eyes.shape, X_val_face.shape, X_val_kinect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2114,)\n"
     ]
    }
   ],
   "source": [
    "y_val = np.load('y_val.npy')\n",
    "\n",
    "print y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2114, 30) (2114, 12) (2114, 40) (2114, 20)\n"
     ]
    }
   ],
   "source": [
    "nn_audio_val = model_audio.predict(X_val_audio)\n",
    "nn_eyes_val = model_eyes.predict(X_val_eyes)\n",
    "nn_face_val = model_face.predict(X_val_face)\n",
    "nn_kinect_val = model_kinect.predict(X_val_kinect)\n",
    "\n",
    "print nn_audio_val.shape, nn_eyes_val.shape, nn_face_val.shape, nn_kinect_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighted Soft Aggregation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_audio = RandomForestClassifier(n_estimators=100)\n",
    "predictor_eyes = RandomForestClassifier(n_estimators=100)\n",
    "predictor_face = RandomForestClassifier(n_estimators=100)\n",
    "predictor_kinect = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio: [ 0.44256121  0.40377358  0.38636364  0.42857143] 0.415317463778\n",
      "eyes: [ 0.39171375  0.29433962  0.26325758  0.26095238] 0.302565831749\n",
      "face: [ 0.58380414  0.45283019  0.60037879  0.6       ] 0.559253279921\n",
      "kinect: [ 0.44256121  0.27735849  0.33712121  0.41904762] 0.369022131752\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(predictor_audio, nn_audio_val, y_val, cv=4)\n",
    "print 'audio:', scores, np.mean(scores)\n",
    "scores = cross_val_score(predictor_eyes, nn_eyes_val, y_val, cv=4)\n",
    "print 'eyes:', scores, np.mean(scores)\n",
    "scores = cross_val_score(predictor_face, nn_face_val, y_val, cv=4)\n",
    "print 'face:', scores, np.mean(scores)\n",
    "scores = cross_val_score(predictor_kinect, nn_kinect_val, y_val, cv=4)\n",
    "print 'kinect:', scores, np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2114, 6) (2114, 6) (2114, 6) (2114, 6)\n"
     ]
    }
   ],
   "source": [
    "weak_audio_val = cross_val_predict(predictor_audio, nn_audio_val, y_val, cv=4, method='predict_proba')\n",
    "weak_eyes_val = cross_val_predict(predictor_eyes, nn_eyes_val, y_val, cv=4, method='predict_proba')\n",
    "weak_face_val = cross_val_predict(predictor_face, nn_face_val, y_val, cv=4, method='predict_proba')\n",
    "weak_kinect_val = cross_val_predict(predictor_kinect, nn_kinect_val, y_val, cv=4, method='predict_proba')\n",
    "\n",
    "print weak_audio_val.shape, weak_eyes_val.shape, weak_face_val.shape, weak_kinect_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_weak = [0.42, 0.3, 0.56, 0.37]\n",
    "\n",
    "# preds_weak = 0.42 * weak_audio_val + 0.3 * weak_eyes_val + 0.56 * weak_face_val + 0.37 * weak_kinect_val\n",
    "preds_weak = weak_audio_val + weak_eyes_val + weak_face_val + weak_kinect_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2114,)\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(preds_weak, axis=1)\n",
    "\n",
    "print y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.450331125828\n"
     ]
    }
   ],
   "source": [
    "print np.mean(y_pred != y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fusion-based**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2114, 102)\n"
     ]
    }
   ],
   "source": [
    "nn_val = np.concatenate([nn_audio_val, nn_eyes_val, nn_face_val, nn_kinect_val], axis=1)\n",
    "\n",
    "print nn_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   6.3s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................................. , total=   6.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................................. , total=   6.1s\n",
      "[ 0.61189802  0.56028369  0.66571835] 0.612633351623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   18.4s finished\n"
     ]
    }
   ],
   "source": [
    "predictor = RandomForestClassifier(n_estimators=1000)\n",
    "\n",
    "scores = cross_val_score(predictor, nn_val, y_val, cv=3, verbose=2)\n",
    "print scores, np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10446, 400, 36) (10446, 200, 6) (10446, 200, 100) (10446, 60, 27)\n"
     ]
    }
   ],
   "source": [
    "X_audio = np.concatenate([norm(np.load('data/X_train_audio.npy'), mins_audio, maxs_audio), \n",
    "                          norm(np.load('data/X_val_audio.npy'), mins_audio, maxs_audio)])\n",
    "X_eyes = np.concatenate([np.load('data/X_train_eyes.npy'), np.load('data/X_val_eyes.npy')])\n",
    "X_face = np.concatenate([np.load('data/X_train_face.npy'), np.load('data/X_val_face.npy')])\n",
    "X_kinect = np.concatenate([norm(np.load('data/X_train_kinect.npy'), mins_kinect, maxs_kinect), \n",
    "                           norm(np.load('data/X_val_kinect.npy'),mins_kinect, maxs_kinect)])\n",
    "\n",
    "print X_audio.shape, X_eyes.shape, X_face.shape, X_kinect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10446,)\n"
     ]
    }
   ],
   "source": [
    "y = np.concatenate([np.load('data/y_train.npy'), np.load('data/y_val.npy')])\n",
    "\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tmp = keras.models.load_model('models/model_audio_phase2.h5')\n",
    "model_audio = keras.models.Model(model_tmp.input, model_tmp.get_layer('dropout_2').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tmp = keras.models.load_model('models/model_eyes_phase2.h5')\n",
    "model_eyes = keras.models.Model(model_tmp.input, model_tmp.get_layer('dropout_2').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tmp = keras.models.load_model('models/model_face_phase2.h5')\n",
    "model_face = keras.models.Model(model_tmp.input, model_tmp.get_layer('dropout_2').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tmp = keras.models.load_model('models/model_kinect_phase2.h5')\n",
    "model_kinect = keras.models.Model(model_tmp.input, model_tmp.get_layer('dropout_2').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10446, 30) (10446, 12) (10446, 40) (10446, 20)\n"
     ]
    }
   ],
   "source": [
    "nn_audio = model_audio.predict(X_audio)\n",
    "nn_eyes = model_eyes.predict(X_eyes)\n",
    "nn_face = model_face.predict(X_face)\n",
    "nn_kinect = model_kinect.predict(X_kinect)\n",
    "\n",
    "print nn_audio.shape, nn_eyes.shape, nn_face.shape, nn_kinect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_audio, model_eyes, model_face, model_kinect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10446, 102)\n"
     ]
    }
   ],
   "source": [
    "nn = np.concatenate([nn_audio, nn_eyes, nn_face, nn_kinect], axis=1)\n",
    "\n",
    "print nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = RandomForestClassifier(n_estimators=1000, n_jobs=4).fit(nn, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/predictor_phase2.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(predictor, 'models/predictor_phase2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
